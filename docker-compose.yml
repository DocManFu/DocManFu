# =============================================================================
# DocManFu Production Docker Compose
# Usage: docker compose up -d  (or ./prod up -d)
# =============================================================================

services:
  db:
    image: postgres:16-alpine
    environment:
      POSTGRES_USER: docmanfu
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:?Set POSTGRES_PASSWORD in .env}
      POSTGRES_DB: docmanfu
    volumes:
      - pgdata:/var/lib/postgresql/data
    shm_size: 256mb
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U docmanfu"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"

  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    volumes:
      - redisdata:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"

  migrate:
    build: .
    command: ["alembic", "upgrade", "head"]
    env_file: .env
    environment:
      DATABASE_URL: postgresql://docmanfu:${POSTGRES_PASSWORD:?}@db:5432/docmanfu
    depends_on:
      db:
        condition: service_healthy
    restart: "no"
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"

  api:
    build: .
    command: >
      uvicorn app.main:app
      --host 0.0.0.0
      --port 8000
      --workers ${API_WORKERS:-2}
    env_file: .env
    environment:
      DATABASE_URL: postgresql://docmanfu:${POSTGRES_PASSWORD:?}@db:5432/docmanfu
      REDIS_URL: redis://redis:6379/0
    volumes:
      - uploads:/app/uploads
    depends_on:
      migrate:
        condition: service_completed_successfully
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"

  worker:
    build: .
    command: >
      celery -A app.core.celery_app:celery_app worker
      --loglevel=info
      --concurrency=${WORKER_CONCURRENCY:-2}
    env_file: .env
    environment:
      DATABASE_URL: postgresql://docmanfu:${POSTGRES_PASSWORD:?}@db:5432/docmanfu
      REDIS_URL: redis://redis:6379/0
    volumes:
      - uploads:/app/uploads
    depends_on:
      migrate:
        condition: service_completed_successfully
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "celery", "-A", "app.core.celery_app:celery_app", "inspect", "ping", "--timeout", "5"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"

  frontend:
    build: ./frontend
    ports:
      - "${FRONTEND_PORT:-8080}:80"
    depends_on:
      api:
        condition: service_healthy
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"

  # Optional: Ollama for local AI (activate with --profile ollama)
  # Requires NVIDIA Container Toolkit on Linux for GPU support
  ollama:
    image: ollama/ollama:latest
    profiles: ["ollama"]
    volumes:
      - ollama_models:/root/.ollama
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"

  # Optional: Ollama without GPU (activate with --profile ollama-cpu)
  ollama-cpu:
    image: ollama/ollama:latest
    profiles: ["ollama-cpu"]
    volumes:
      - ollama_models:/root/.ollama
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "5"

volumes:
  pgdata:
  redisdata:
  uploads:
  ollama_models:
